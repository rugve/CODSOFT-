{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH8zfuizhmru",
        "outputId": "ec092018-2302-4f5a-82e7-30993cf6cb2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a man standing in the middle of a field']\n"
          ]
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class ComplexImageCaptioningModel:\n",
        "    def __init__(self, model_name=\"nlpconnect/vit-gpt2-image-captioning\"):\n",
        "        self.model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "        self.feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.max_length = 16\n",
        "        self.num_beams = 4\n",
        "        self.gen_kwargs = {\"max_length\": self.max_length, \"num_beams\": self.num_beams}\n",
        "\n",
        "    def preprocess_images(self, image_paths):\n",
        "        images = []\n",
        "        for image_path in image_paths:\n",
        "            i_image = Image.open(image_path)\n",
        "            if i_image.mode != \"RGB\":\n",
        "                i_image = i_image.convert(mode=\"RGB\")\n",
        "            images.append(i_image)\n",
        "        return images\n",
        "\n",
        "    def generate_captions(self, images):\n",
        "        pixel_values = self.feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
        "        pixel_values = pixel_values.to(self.device)\n",
        "\n",
        "        output_ids = self.model.generate(pixel_values, **self.gen_kwargs)\n",
        "\n",
        "        preds = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "        preds = [pred.strip() for pred in preds]\n",
        "        return preds\n",
        "\n",
        "    def predict_step(self, image_paths):\n",
        "        try:\n",
        "            images = self.preprocess_images(image_paths)\n",
        "            captions = self.generate_captions(images)\n",
        "            return captions\n",
        "        except Exception as e:\n",
        "            print(f\"Error during prediction: {e}\")\n",
        "            return []\n",
        "\n",
        "# Example usage\n",
        "model_instance = ComplexImageCaptioningModel()\n",
        "\n",
        "result = model_instance.predict_step(['/content/images-1.jpeg'])\n",
        "print(result)\n"
      ]
    }
  ]
}